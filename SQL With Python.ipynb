{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff536f4",
   "metadata": {},
   "source": [
    "# SQL Queries and Data Visualization #\n",
    " \n",
    "Welcome to this SQL tutorial using Python. Our focus here is on executing SQL queries, and for this purpose, we've set up a local database using SQLite3. SQL stands out for its efficiency in handling data queries. Additionally, we'll demonstrate how to integrate our database with a data visualization tool like PowerBI, enriching our analysis capabilities.\n",
    "\n",
    "In this tutorial, we use various time series datasets: (1) Job destruction data from the Census' Business Dynamics Statistics; (2) The U.S. GDP, unemployment, and mortgage interest rates from the Federal Reserve Bank of St. Louis; (3) Average personal consumption and income by state from the U.S. Bureau of Economic Analysis; (3) Natality data by state and age group from the Centers for Disease Control and Prevention; (4) Mass layoff notices from the Federal Reserve Bank of Cleveland.\n",
    "\n",
    "Our goal is to retrieve, process, and utilize this data effectively. Once we format the data as required, we'll showcase how to use it with PowerBI to create an insightful dashboard.\n",
    "\n",
    "## Connecting to the Database ##\n",
    "\n",
    "The first step is to establish a connection with our database and explore its structure, including the available tables and their respective columns. We'll begin by importing the necessary libraries and then connect to the SQLite database. Following this, we execute a SQL query to list all the available tables in the database. This query will return a set of rows, where each row contains the name of one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d99a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70884275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bds_state', 'gdp_us', 'mortagage_us', 'cons_state', 'inc_state', 'unemployment_us', 'natality_state', 'warn_state_data', 'combined_state_data']\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('C:\\GitHub\\labor_market_natality.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "conn.close()\n",
    "\n",
    "table_names = [table[0] for table in tables]\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d0c9b",
   "metadata": {},
   "source": [
    "## Checking Columns ##\n",
    "\n",
    "In this section, we will develop a Python function that retrieves the names and data types of columns for each table in our database. This understanding is essential for determining how tables can be combined and which transformations are necessary to obtain meaningful insights.\n",
    "\n",
    "## Function to Retrieve Column Information ##\n",
    "\n",
    "We'll create a function named get_column_info. This function will take the name of a table as an input and return a list of tuples, each containing the name and data type of a column in that table. This approach provides a clear view of the table's structure, aiding in our data analysis and preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc5f697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(table):\n",
    "    conn = sqlite3.connect('C:\\GitHub\\labor_market_natality.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('PRAGMA table_info('+table+')')\n",
    "    columns = cursor.fetchall()\n",
    "    conn.close()\n",
    "    column_names = [column[1] for column in columns]\n",
    "    column_types = [column[2] for column in columns]\n",
    "    print(column_names)\n",
    "    print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56e2eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'statefips', 'year', 'ajd_total', 'jdd_total', 'ajd_female', 'ajd_male', 'jdd_female', 'jdd_male', 'pop_total', 'pop_female', 'pop_male']\n",
      "['INTEGER', 'REAL', 'INTEGER', 'INTEGER', 'INTEGER', 'REAL', 'REAL', 'REAL', 'REAL', 'INTEGER', 'INTEGER', 'INTEGER']\n"
     ]
    }
   ],
   "source": [
    "check_columns('bds_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a66afe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'statefips', 'year', 'month', 'age', 'births', 'quarter']\n",
      "['INTEGER', 'INTEGER', 'INTEGER', 'INTEGER', 'TEXT', 'TEXT', 'TEXT']\n"
     ]
    }
   ],
   "source": [
    "check_columns('natality_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be530610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'month', 'year', 'quarter', 'statefips', 'layoffs']\n",
      "['INTEGER', 'INTEGER', 'TEXT', 'TEXT', 'TEXT', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "check_columns('warn_state_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "149b6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statefips', 'year', 'durable_exp', 'all_exp']\n",
      "['REAL', 'TEXT', 'REAL', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "check_columns('cons_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c361759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statefips', 'year', 'quarter', 'personal', 'population']\n",
      "['REAL', 'REAL', 'REAL', 'INTEGER', 'INTEGER']\n"
     ]
    }
   ],
   "source": [
    "check_columns('inc_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e2570",
   "metadata": {},
   "source": [
    "## Combining and Transforming Data ##\n",
    "\n",
    "With the column information at our disposal, we can strategically decide how to combine data from different tables. This step is crucial for creating a comprehensive dataset that can be effectively used in data visualization tools like PowerBI.\n",
    "\n",
    "We will use SQL commands to create a new table in our database. This table will be specifically structured to hold the combined and transformed data, optimizing it for visualization purposes. We'll ensure that the table schema (i.e., frequency, column names and data types) aligns with the requirements of our analysis and the capabilities of PowerBI.\n",
    "\n",
    "## Connecting PowerBI to the SQLite Database ##\n",
    "\n",
    "With our new table ready, the next crucial step is to establish a connection between PowerBI and our SQLite database. This direct connection enables us to use the live data from the database, facilitating real-time analysis and interactive dashboard creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fac39",
   "metadata": {},
   "source": [
    "## Creating Yearly Time Series ##\n",
    "\n",
    "Having gained detailed insights into the structure of our state-level data tables, we've identified a challenge: the data frequencies vary across different tables. For instance, WARN notices and natality data are available monthly, whereas personal income data is quarterly, and both job destruction and per-capita consumption data are presented annually. To effectively analyze and visualize these datasets together, we need to standardize their frequencies to a yearly basis.\n",
    "\n",
    "## Standardizing Data Frequencies ##\n",
    "\n",
    "(1) Monthly Data (WARN Notices and Natality): We will aggregate these monthly datasets by summing the data for each year. This process will give us the total annual figures for WARN notices and natality, allowing for a yearly comparison; (2) Quarterly Data (Personal Income): Since this dataset is already presented as a rate, we will calculate the annual average. This method ensures that we capture the typical income trend for each year, smoothing out any seasonal variations; (3) Yearly Data (Job Destruction and Per-Capita Consumption): These datasets are already in the desired yearly format and will be used as is in our analysis.\n",
    "\n",
    "## Combining Datasets for a Unified View ##\n",
    "\n",
    "After processing the individual datasets, our next step is to combine them into a single, comprehensive yearly dataset. This unified dataset will include yearly figures for natality, WARN notices, personal income, job destruction, and per-capita consumption.\n",
    "\n",
    "We start by processing and combining the WARN notices and natality data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f3e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r'C:\\GitHub\\labor_market_natality.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fb1aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "natality_data = \"\"\"\n",
    "    (SELECT statefips, year, SUM(births) as total_births \n",
    "    FROM natality_state \n",
    "    GROUP BY statefips, year) n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae676414",
   "metadata": {},
   "outputs": [],
   "source": [
    "warn_data = \"\"\"\n",
    "    (SELECT statefips, year, SUM(layoffs) as total_layoffs \n",
    "    FROM warn_state_data \n",
    "    GROUP BY statefips, year) w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d9f7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE combined_state_data AS \n",
    "    SELECT \n",
    "        n.statefips, \n",
    "        n.year, \n",
    "        n.total_births, \n",
    "        w.total_layoffs \n",
    "    FROM {natality_data} \n",
    "    LEFT JOIN {warn_data} \n",
    "    ON n.statefips = w.statefips AND n.year = w.year\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baa4238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x20954381140>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff7942",
   "metadata": {},
   "source": [
    "Now, we turn our attention to the income data, applying a similar process of transformation and integration. The goal here is to refine the income data into a state-by-year format, aligning it with the structure of the dataset we've already started building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1c9f69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x20954381140>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"ALTER TABLE combined_state_data ADD COLUMN total_pop INTEGER;\")\n",
    "cursor.execute(\"ALTER TABLE combined_state_data ADD COLUMN avg_personal_inc REAL;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bb66aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_data = \"\"\"\n",
    "    SELECT statefips, year, AVG(population) as total_pop, AVG(personal) as avg_personal_inc \n",
    "    FROM inc_state \n",
    "    GROUP BY statefips, year\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e79a96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bring_inc_data = f\"\"\"\n",
    "    UPDATE combined_state_data\n",
    "    SET \n",
    "        total_pop = (SELECT w.total_pop \n",
    "            FROM ({inc_data}) w \n",
    "            WHERE combined_state_data.statefips = w.statefips AND combined_state_data.year = w.year),\n",
    "            \n",
    "        avg_personal_inc = (SELECT w.avg_personal_inc \n",
    "            FROM ({inc_data}) w \n",
    "            WHERE combined_state_data.statefips = w.statefips AND combined_state_data.year = w.year);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52fd50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x20954381140>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(bring_inc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21d2ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x20954381140>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"ALTER TABLE combined_state_data ADD COLUMN all_exp REAL;\")\n",
    "cursor.execute(\"ALTER TABLE combined_state_data ADD COLUMN durable_exp REAL;\")\n",
    "cursor.execute(\"ALTER TABLE combined_state_data ADD COLUMN ajd_total REAL;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73578868",
   "metadata": {},
   "outputs": [],
   "source": [
    "bring_yearly_series = \"\"\"\n",
    "    UPDATE combined_state_data\n",
    "    SET \n",
    "        durable_exp = (SELECT w.durable_exp \n",
    "            FROM cons_state w \n",
    "            WHERE combined_state_data.statefips = w.statefips AND combined_state_data.year = w.year),\n",
    "        all_exp = (SELECT w.all_exp \n",
    "            FROM cons_state w \n",
    "            WHERE combined_state_data.statefips = w.statefips AND combined_state_data.year = w.year),\n",
    "        ajd_total = (SELECT w.ajd_total \n",
    "            FROM bds_state w \n",
    "            WHERE combined_state_data.statefips = w.statefips AND combined_state_data.year = w.year);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8a306a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x20954381140>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(bring_yearly_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee68fac",
   "metadata": {},
   "source": [
    "With all the state-level data now combined into a single table, our next step is to conduct a thorough verification of this new dataset. This process is critical to ensure that our data integration efforts have resulted in an accurate and reliable dataset. We will transform our newly created SQL table into a Pandas DataFrame. This transformation makes it easier to visually inspect the data and perform any necessary checks. By examining the DataFrame, we can quickly assess whether the data has been combined correctly. We'll look for inconsistencies, alignment of data across different years and states, and any anomalies that might indicate issues in the data processing or merging steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e597af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statefips</th>\n",
       "      <th>year</th>\n",
       "      <th>total_births</th>\n",
       "      <th>total_layoffs</th>\n",
       "      <th>total_pop</th>\n",
       "      <th>avg_personal_inc</th>\n",
       "      <th>all_exp</th>\n",
       "      <th>durable_exp</th>\n",
       "      <th>ajd_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>59518.0</td>\n",
       "      <td>12704.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102106.6</td>\n",
       "      <td>13427.5</td>\n",
       "      <td>-3886.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>59460.0</td>\n",
       "      <td>8961.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108428.7</td>\n",
       "      <td>14203.4</td>\n",
       "      <td>-4794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>60399.0</td>\n",
       "      <td>5530.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115631.1</td>\n",
       "      <td>14885.3</td>\n",
       "      <td>-7282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>63165.0</td>\n",
       "      <td>9734.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121487.4</td>\n",
       "      <td>15346.7</td>\n",
       "      <td>-9176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>64733.0</td>\n",
       "      <td>10342.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126916.4</td>\n",
       "      <td>15862.2</td>\n",
       "      <td>-37928.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>56</td>\n",
       "      <td>2018</td>\n",
       "      <td>6523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575109.50</td>\n",
       "      <td>59163.25</td>\n",
       "      <td>24652.3</td>\n",
       "      <td>2602.9</td>\n",
       "      <td>2467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>56</td>\n",
       "      <td>2019</td>\n",
       "      <td>6517.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575605.50</td>\n",
       "      <td>61834.75</td>\n",
       "      <td>25235.7</td>\n",
       "      <td>2621.4</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>56</td>\n",
       "      <td>2020</td>\n",
       "      <td>6084.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577557.00</td>\n",
       "      <td>65562.75</td>\n",
       "      <td>25579.8</td>\n",
       "      <td>2783.0</td>\n",
       "      <td>-655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>56</td>\n",
       "      <td>2021</td>\n",
       "      <td>6229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>579429.75</td>\n",
       "      <td>70529.25</td>\n",
       "      <td>28312.5</td>\n",
       "      <td>3278.2</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>56</td>\n",
       "      <td>2022</td>\n",
       "      <td>6008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>581507.25</td>\n",
       "      <td>73229.75</td>\n",
       "      <td>30465.8</td>\n",
       "      <td>3459.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      statefips  year  total_births  total_layoffs  total_pop  \\\n",
       "0             1  2003       59518.0        12704.0        NaN   \n",
       "1             1  2004       59460.0         8961.0        NaN   \n",
       "2             1  2005       60399.0         5530.0        NaN   \n",
       "3             1  2006       63165.0         9734.0        NaN   \n",
       "4             1  2007       64733.0        10342.0        NaN   \n",
       "...         ...   ...           ...            ...        ...   \n",
       "1015         56  2018        6523.0            NaN  575109.50   \n",
       "1016         56  2019        6517.0            NaN  575605.50   \n",
       "1017         56  2020        6084.0            NaN  577557.00   \n",
       "1018         56  2021        6229.0            NaN  579429.75   \n",
       "1019         56  2022        6008.0            NaN  581507.25   \n",
       "\n",
       "      avg_personal_inc   all_exp  durable_exp  ajd_total  \n",
       "0                  NaN  102106.6      13427.5    -3886.0  \n",
       "1                  NaN  108428.7      14203.4    -4794.0  \n",
       "2                  NaN  115631.1      14885.3    -7282.0  \n",
       "3                  NaN  121487.4      15346.7    -9176.0  \n",
       "4                  NaN  126916.4      15862.2   -37928.0  \n",
       "...                ...       ...          ...        ...  \n",
       "1015          59163.25   24652.3       2602.9     2467.0  \n",
       "1016          61834.75   25235.7       2621.4      397.0  \n",
       "1017          65562.75   25579.8       2783.0     -655.0  \n",
       "1018          70529.25   28312.5       3278.2       31.0  \n",
       "1019          73229.75   30465.8       3459.6        0.0  \n",
       "\n",
       "[1020 rows x 9 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT * FROM combined_state_data;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e1e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdef228",
   "metadata": {},
   "source": [
    "## Moving to PowerBI ##\n",
    "\n",
    "With our comprehensive table of different time series data ready, the next exciting step is to utilize PowerBI for creating a sample dashboard. PowerBI's robust visualization capabilities will allow us to explore and present our data in dynamic and insightful ways.\n",
    "\n",
    "To access SQLite3 tables in PowerBI, the first step is to install an ODBC driver. This driver acts as a bridge between SQLite and PowerBI, enabling them to communicate effectively. You can download a suitable driver (for example, http://www.ch-werner.de/sqliteodbc). Once the driver is installed, start PowerBI and click on \"Get Data\".\n",
    "\n",
    "    Selecting the Data Source: C\n",
    "    Setting Up the Connection: \n",
    "\n",
    "    Authentication: \n",
    "\n",
    "    Selecting the Table: \n",
    "\n",
    "![step1](Step1.png)\n",
    "\n",
    "\n",
    "Then choose \"More\" to view all data sources and then find \"ODBC\" in the list. Upon selecting \"ODBC\", choose \"SQLite3 Datasource\" and proceed to \"Advanced options\".\n",
    "\n",
    "\n",
    "![step2](Step2A.png)\n",
    "\n",
    "\n",
    "In the \"Connection string\" field, enter the path to the SQLite3 database you created. For example, if your database is located at C:\\GitHub\\labor_market_natality.db, your connection string would be database=C:\\GitHub\\labor_market_natality.db.\n",
    "\n",
    "\n",
    "![step3](Step3.png)\n",
    "\n",
    "![step4](Step4.png)\n",
    "\n",
    "\n",
    "When prompted for credentials, select 'current credentials'. PowerBI will then display a list of tables from your database. For this tutorial, select the 'combined_state_data' table, which is the one we prepared earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f69a13",
   "metadata": {},
   "source": [
    "Now, let's dive into the process of creating a data visualization in PowerBI. We will start by creating a line chart. To do that, select the \"Line Chart\" option. This type of chart is particularly effective for displaying trends over time. For a time series plot, the X-axis typically represents the time variable. In our case, drag the \"year\" variable from your dataset to the X-axis field in the line chart settings. This will set up the foundation of our plot, showing the progression of time on the horizontal axis. The Y-axis will represent the data we want to analyze over time. For example, if you want to visualize trends in durable goods, drag the \"durable goods\" variable to the Y-axis field. This will plot the values of durable goods against the corresponding years, allowing us to observe changes and trends in this variable over time.\n",
    "\n",
    "![step6](Step6.png)\n",
    "\n",
    "To further refine your visualization in PowerBI and make it more user-friendly, you can customize labels, formats, and filters. Right-click on the variable in the Y-axis field and select \"Rename for this visual\". For more detailed customization, like changing titles, adjusting colors, or modifying other general parameters, navigate to \"Format your visual\". This section in PowerBI offers a range of options to tailor your chart’s appearance, including font sizes, line styles, and color palettes. To focus on a specific time period, such as 2010-2020, or to filter the data in any way, use the filter options. In the \"Filters on this visual\" section, add a filter, for example, for the state variable. This allows you to isolate and explore data relevant to a particular state, offering more targeted insights.\n",
    "\n",
    "![step7](Step7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353cd11",
   "metadata": {},
   "source": [
    "After following the outlined steps, I successfully created a basic dashboard in PowerBI that showcases some aspects of our dataset. Since our data is organized by state and year, I specifically focused on creating these visualizations for California, Texas, and New York. This state-level analysis allows for a more detailed examination of trends and patterns within these specific regions. The first plot illustrates the relationship between the number of births and spending on durable goods over time. The second time series plot compares the number of births with the total population. I also created a scatterplot that explores the relationship between spending on durable goods and the incidence of mass layoffs. This visualization can reveal interesting patterns or correlations between economic conditions and employment dynamics.\n",
    "\n",
    "![step8](Step8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb1f27",
   "metadata": {},
   "source": [
    "This tutorial provides a concise yet comprehensive introduction to utilizing SQL and PowerBI for data analysis and visualization. Throughout this guide, we've covered the basics of retrieving and processing data using SQL, followed by steps to leverage this data for creating insightful visualizations in PowerBI. To facilitate further learning and experimentation, the Jupyter notebook used in this tutorial, along with all the data files, are available for download. The .pbix file, which includes the PowerBI dashboard created in this tutorial, is also accessible. All these resources can be found in the GitHub repository, enabling you to explore, modify, and expand upon what we've discussed here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
